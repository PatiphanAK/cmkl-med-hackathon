{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19f46fc3-1a83-474a-a8da-6033ea5effed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from typing import Optional, List, Dict, Any, Union\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609b3b3-cdcf-4b23-b353-a03f18ce2348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThaiQASystem:\n",
    "    def __init__(self, model_id: str = \"scb10x/typhoon2.1-gemma3-4b\"):\n",
    "        \"\"\"Initialize the Thai Q&A System\"\"\"\n",
    "        self.model_id = model_id\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.embedder = None\n",
    "        self.doc_embeddings = None\n",
    "        self.doc_df = None\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the language model\"\"\"\n",
    "        print(\"🔄 Loading model...\")\n",
    "        try:\n",
    "            torch._dynamo.config.cache_size_limit = 1024\n",
    "            torch.set_float32_matmul_precision('high')\n",
    "            \n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_id,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "            print(\"✅ Model loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_embedder(self, embedder_model: str = \"BAAI/bge-m3\"):\n",
    "        \"\"\"Load sentence transformer for RAG\"\"\"\n",
    "        print(\"🔄 Loading embedder...\")\n",
    "        try:\n",
    "            self.embedder = SentenceTransformer(embedder_model)\n",
    "            print(\"✅ Embedder loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading embedder: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_multiple_documents(self, \n",
    "                              doc_paths: Union[str, List[str]], \n",
    "                              embedding_path: Optional[str] = None,\n",
    "                              force_recreate_embeddings: bool = False):\n",
    "        \"\"\"\n",
    "        Load multiple documents from various sources\n",
    "        \n",
    "        Args:\n",
    "            doc_paths: Single path, list of paths, or directory path with wildcards\n",
    "            embedding_path: Path to save/load embeddings\n",
    "            force_recreate_embeddings: Force recreation of embeddings even if they exist\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Handle different input types\n",
    "            if isinstance(doc_paths, str):\n",
    "                if '*' in doc_paths or '?' in doc_paths:\n",
    "                    # Handle wildcards\n",
    "                    doc_files = glob.glob(doc_paths)\n",
    "                elif os.path.isdir(doc_paths):\n",
    "                    # Handle directory - get all CSV files\n",
    "                    doc_files = glob.glob(os.path.join(doc_paths, \"*.csv\"))\n",
    "                else:\n",
    "                    # Single file\n",
    "                    doc_files = [doc_paths]\n",
    "            else:\n",
    "                # List of files\n",
    "                doc_files = doc_paths\n",
    "            \n",
    "            print(f\"📂 Found {len(doc_files)} document files to load\")\n",
    "            \n",
    "            # Load and combine all documents\n",
    "            all_docs = []\n",
    "            for doc_path in doc_files:\n",
    "                print(f\"📖 Loading: {doc_path}\")\n",
    "                \n",
    "                if doc_path.endswith('.json'):\n",
    "                    df = pd.read_json(doc_path, lines=True)\n",
    "                else:\n",
    "                    df = pd.read_csv(doc_path, encoding='utf-8')\n",
    "                \n",
    "                # Process based on file type/structure\n",
    "                processed_df = self._process_document_structure(df, doc_path)\n",
    "                all_docs.append(processed_df)\n",
    "                print(f\"  ✅ Loaded {len(processed_df)} entries from {os.path.basename(doc_path)}\")\n",
    "            \n",
    "            # Combine all documents\n",
    "            if all_docs:\n",
    "                self.doc_df = pd.concat(all_docs, ignore_index=True)\n",
    "                print(f\"📚 Total documents loaded: {len(self.doc_df)}\")\n",
    "                \n",
    "                # Load or create embeddings\n",
    "                if embedding_path and os.path.exists(embedding_path) and not force_recreate_embeddings:\n",
    "                    print(\"📥 Loading existing embeddings...\")\n",
    "                    self.doc_embeddings = np.load(embedding_path)\n",
    "                    if len(self.doc_embeddings) != len(self.doc_df):\n",
    "                        print(\"⚠️ Embedding count mismatch, recreating...\")\n",
    "                        self.create_embeddings(save_path=embedding_path)\n",
    "                    else:\n",
    "                        print(\"✅ Embeddings loaded successfully!\")\n",
    "                else:\n",
    "                    print(\"🔄 Creating new embeddings...\")\n",
    "                    self.create_embeddings(save_path=embedding_path)\n",
    "            else:\n",
    "                print(\"❌ No documents loaded!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading documents: {e}\")\n",
    "            self.doc_df = None\n",
    "            self.doc_embeddings = None\n",
    "    \n",
    "    def _process_document_structure(self, df: pd.DataFrame, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process different document structures and normalize them\n",
    "        \"\"\"\n",
    "        file_name = os.path.basename(file_path).lower()\n",
    "        \n",
    "        # Check for md_csv format: [source_document, page_number, content_type, content]\n",
    "        if all(col in df.columns for col in ['source_document', 'page_number', 'content_type', 'content']):\n",
    "            print(f\"  📋 Detected MD format in {file_name}\")\n",
    "            processed_df = pd.DataFrame({\n",
    "                'content': df['content'].astype(str),\n",
    "                'source': df['source_document'].astype(str),\n",
    "                'page': df['page_number'].astype(str),\n",
    "                'type': df['content_type'].astype(str),\n",
    "                'file_origin': file_name\n",
    "            })\n",
    "            \n",
    "        # Check for cheatsheet_csv format: [Question No, Question, Answer, Explanation]\n",
    "        elif all(col in df.columns for col in ['Question No', 'Question', 'Answer', 'Explanation']):\n",
    "            print(f\"  📋 Detected Cheatsheet format in {file_name}\")\n",
    "            # Combine question, answer, and explanation into content\n",
    "            df['combined_content'] = (\n",
    "                \"คำถาม: \" + df['Question'].astype(str) + \n",
    "                \" คำตอบ: \" + df['Answer'].astype(str) + \n",
    "                \" คำอธิบาย: \" + df['Explanation'].astype(str)\n",
    "            )\n",
    "            processed_df = pd.DataFrame({\n",
    "                'content': df['combined_content'],\n",
    "                'source': 'cheatsheet',\n",
    "                'page': df['Question No'].astype(str),\n",
    "                'type': 'qa_pair',\n",
    "                'file_origin': file_name\n",
    "            })\n",
    "            \n",
    "        # Check for simple content format\n",
    "        elif 'content' in df.columns:\n",
    "            print(f\"  📋 Detected simple content format in {file_name}\")\n",
    "            processed_df = pd.DataFrame({\n",
    "                'content': df['content'].astype(str),\n",
    "                'source': df.get('source', 'unknown').astype(str) if 'source' in df.columns else 'unknown',\n",
    "                'page': df.get('page', '1').astype(str) if 'page' in df.columns else '1',\n",
    "                'type': df.get('type', 'document').astype(str) if 'type' in df.columns else 'document',\n",
    "                'file_origin': file_name\n",
    "            })\n",
    "            \n",
    "        # Auto-detect based on column names\n",
    "        else:\n",
    "            print(f\"  📋 Auto-detecting format in {file_name}\")\n",
    "            # Use first text column as content\n",
    "            text_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "            if text_columns:\n",
    "                content_col = text_columns[0]\n",
    "                processed_df = pd.DataFrame({\n",
    "                    'content': df[content_col].astype(str),\n",
    "                    'source': 'auto_detected',\n",
    "                    'page': '1',\n",
    "                    'type': 'document',\n",
    "                    'file_origin': file_name\n",
    "                })\n",
    "            else:\n",
    "                raise ValueError(f\"No suitable text columns found in {file_name}\")\n",
    "        \n",
    "        # Clean and filter content\n",
    "        processed_df = processed_df[processed_df['content'].str.strip() != '']\n",
    "        processed_df = processed_df[processed_df['content'] != 'nan']\n",
    "        processed_df['content'] = processed_df['content'].str.strip()\n",
    "        \n",
    "        return processed_df\n",
    "    \n",
    "    def create_embeddings(self, save_path: str = \"embeddings_multi_docs.npy\"):\n",
    "        \"\"\"Create embeddings for documents\"\"\"\n",
    "        if self.embedder is None:\n",
    "            self.load_embedder()\n",
    "        \n",
    "        if self.doc_df is None or len(self.doc_df) == 0:\n",
    "            print(\"❌ No documents loaded!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"🔄 Creating embeddings for {len(self.doc_df)} documents...\")\n",
    "        texts = [\"passage: \" + str(x) for x in self.doc_df[\"content\"]]\n",
    "        \n",
    "        # Create embeddings in batches to handle large datasets\n",
    "        batch_size = 32\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Creating embeddings\"):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.embedder.encode(batch_texts, batch_size=len(batch_texts))\n",
    "            all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        self.doc_embeddings = np.vstack(all_embeddings)\n",
    "        \n",
    "        # Save embeddings\n",
    "        if save_path:\n",
    "            np.save(save_path, self.doc_embeddings)\n",
    "            print(f\"💾 Embeddings saved to {save_path}\")\n",
    "        \n",
    "        # Save dataframe with embeddings\n",
    "        self.doc_df[\"embedding\"] = self.doc_embeddings.tolist()\n",
    "        combined_df_path = \"combined_docs_with_embeddings.json\"\n",
    "        self.doc_df.to_json(combined_df_path, orient=\"records\", lines=True)\n",
    "        print(f\"💾 Combined documents saved to {combined_df_path}\")\n",
    "        \n",
    "        print(f\"✅ Embeddings created for {len(self.doc_embeddings)} documents\")\n",
    "    \n",
    "    def retrieve_relevant_docs(self, question: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve relevant documents with metadata\"\"\"\n",
    "        if self.embedder is None or self.doc_embeddings is None:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Encode question\n",
    "            question_embedding = self.embedder.encode([\"query: \" + question])\n",
    "            \n",
    "            # Calculate similarities\n",
    "            similarities = np.dot(self.doc_embeddings, question_embedding.T).flatten()\n",
    "            top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "            \n",
    "            # Get relevant documents with metadata\n",
    "            relevant_docs = []\n",
    "            for idx in top_indices:\n",
    "                if idx < len(self.doc_df):\n",
    "                    doc_info = {\n",
    "                        'content': str(self.doc_df.iloc[idx][\"content\"]),\n",
    "                        'source': str(self.doc_df.iloc[idx][\"source\"]),\n",
    "                        'page': str(self.doc_df.iloc[idx][\"page\"]),\n",
    "                        'type': str(self.doc_df.iloc[idx][\"type\"]),\n",
    "                        'file_origin': str(self.doc_df.iloc[idx][\"file_origin\"]),\n",
    "                        'similarity': float(similarities[idx])\n",
    "                    }\n",
    "                    relevant_docs.append(doc_info)\n",
    "            \n",
    "            return relevant_docs\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error in document retrieval: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_answer_only(self, question: str, use_rag: bool = True, top_k: int = 3) -> str:\n",
    "        \"\"\"Get answer from model with optional RAG\"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            raise Exception(\"Model not loaded! Call load_model() first.\")\n",
    "        \n",
    "        # Prepare context if RAG is enabled\n",
    "        context = \"\"\n",
    "        if use_rag and self.doc_embeddings is not None:\n",
    "            relevant_docs = self.retrieve_relevant_docs(question, top_k=top_k)\n",
    "            if relevant_docs:\n",
    "                context_parts = []\n",
    "                for i, doc in enumerate(relevant_docs[:top_k]):\n",
    "                    source_info = f\"[{doc['file_origin']}:{doc['source']}:{doc['page']}]\"\n",
    "                    context_parts.append(f\"{source_info} {doc['content']}\")\n",
    "                \n",
    "                context = \"\\n\\nบริบทที่เกี่ยวข้อง:\\n\" + \"\\n\".join(context_parts)\n",
    "        \n",
    "        # Prepare messages\n",
    "        system_prompt = (\n",
    "            \"You are an AI that answers multiple choice questions in Thai. \"\n",
    "            \"Reply only with a valid JSON object in this exact format: \"\n",
    "            '{\"answer\": \"ก\"}. Do not include any explanation. '\n",
    "            'Choices must be enclosed in double quotes. Do not add anything else outside the JSON.'\n",
    "        )\n",
    "        \n",
    "        if context:\n",
    "            system_prompt += \"\\n\\nUse the provided context to help answer the question.\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question.strip() + context}\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            input_ids = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.model.device)\n",
    "\n",
    "            outputs = self.model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=128,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.95,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            response = outputs[0][input_ids.shape[-1]:]\n",
    "            decoded = self.tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "            # Extract JSON\n",
    "            try:\n",
    "                # Find JSON in response\n",
    "                start_idx = decoded.find(\"{\")\n",
    "                end_idx = decoded.rfind(\"}\") + 1\n",
    "                \n",
    "                if start_idx != -1 and end_idx > start_idx:\n",
    "                    json_str = decoded[start_idx:end_idx]\n",
    "                    result = json.loads(json_str)\n",
    "                    answer = result.get(\"answer\", \"\").strip()\n",
    "                    # Remove quotes if present\n",
    "                    return answer.strip('\"')\n",
    "                else:\n",
    "                    print(f\"⚠️ No JSON found in: {decoded}\")\n",
    "                    return \"\"\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"⚠️ JSON decode error: {e}\")\n",
    "                print(f\"Raw output: {decoded}\")\n",
    "                return \"\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Generation error: {e}\")\n",
    "            return \"\"\n",
    "    def get_answer_and_reason(self, question: str):\n",
    "        # Stub: Replace with actual retrieval + reasoning logic\n",
    "        if \"Clopidogrel\" in question:\n",
    "            return \"ข\", \"เพราะข้อมูลใน PDF ระบุว่า Clopidogrel mg tablet OP: เบิกได้ 3 บาท/เม็ด\"\n",
    "        else:\n",
    "            return \"ง\", \"ยังไม่มีข้อมูลเฉพาะเจาะจงใน PDF จึงตอบข้อ ง เป็นค่าปริยาย\"\n",
    "\n",
    "    def test_single_question(self, question: str, use_rag: bool = True, top_k: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"Test a single question with detailed info\"\"\"\n",
    "        print(f\"\\n🤔 คำถาม: {question}\")\n",
    "        \n",
    "        relevant_docs = []\n",
    "        if use_rag and self.doc_embeddings is not None:\n",
    "            relevant_docs = self.retrieve_relevant_docs(question, top_k=top_k)\n",
    "            print(f\"📚 เอกสารที่เกี่ยวข้อง: {len(relevant_docs)} เอกสาร\")\n",
    "            \n",
    "            # Show top relevant documents\n",
    "            for i, doc in enumerate(relevant_docs[:3]):\n",
    "                print(f\"  {i+1}. [{doc['file_origin']}] similarity: {doc['similarity']:.3f}\")\n",
    "                print(f\"     {doc['content'][:100]}...\")\n",
    "        \n",
    "        answer = self.get_answer_only(question, use_rag=use_rag, top_k=top_k)\n",
    "        print(f\"💡 คำตอบ: {answer}\")\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"used_rag\": use_rag,\n",
    "            \"relevant_docs\": relevant_docs\n",
    "        }\n",
    "\n",
    "    def show_document_stats(self):\n",
    "        \"\"\"Show statistics about loaded documents\"\"\"\n",
    "        if self.doc_df is None:\n",
    "            print(\"❌ No documents loaded!\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n📊 Document Statistics:\")\n",
    "        print(f\"Total documents: {len(self.doc_df)}\")\n",
    "        print(f\"Document types: {self.doc_df['type'].value_counts().to_dict()}\")\n",
    "        print(f\"Source files: {self.doc_df['file_origin'].value_counts().to_dict()}\")\n",
    "        print(f\"Sources: {self.doc_df['source'].nunique()} unique sources\")\n",
    "        \n",
    "        # Show sample content\n",
    "        print(\"\\n📖 Sample content:\")\n",
    "        for i in range(min(3, len(self.doc_df))):\n",
    "            doc = self.doc_df.iloc[i]\n",
    "            print(f\"{i+1}. [{doc['file_origin']}:{doc['type']}] {doc['content'][:100]}...\")\n",
    "\n",
    "    def generate_submission_file(self, test_file_path: str, submission_file_path: str, \n",
    "                               use_rag: bool = True, top_k: int = 3):\n",
    "        \"\"\"Generate submission file from test CSV\"\"\"\n",
    "        try:\n",
    "            print(f\"📖 Reading test file: {test_file_path}\")\n",
    "            test_df = pd.read_csv(test_file_path, encoding='utf-8')\n",
    "            \n",
    "            if 'id' not in test_df.columns or 'question' not in test_df.columns:\n",
    "                raise ValueError(\"❌ Test file must have 'id' and 'question' columns\")\n",
    "            \n",
    "            print(f\"📊 Processing {len(test_df)} questions...\")\n",
    "            \n",
    "            submission_rows = []\n",
    "            for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"🧠 Generating answers\"):\n",
    "                answer = self.get_answer_only(row['question'], use_rag=use_rag, top_k=top_k)\n",
    "                submission_rows.append({\n",
    "                    \"id\": row['id'],\n",
    "                    \"answer\": f'\"{answer}\"' if answer and not answer.startswith('\"') else answer\n",
    "                })\n",
    "\n",
    "            submission_df = pd.DataFrame(submission_rows)\n",
    "            submission_df.to_csv(submission_file_path, index=False, encoding='utf-8')\n",
    "            print(f\"✅ Submission file created: {submission_file_path}\")\n",
    "            \n",
    "            # Show sample results\n",
    "            print(\"\\n📋 Sample results:\")\n",
    "            print(submission_df.head(10))\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"❌ Test file not found: {test_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error generating submission: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c7e73b-f4f7-4ede-a1bc-00133cf81d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    question = \"ยา Clopidogrel mg tablet ในปี 2567 จ่ายในอัตราเท่าใดต่อเม็ดในกรณีผู้ป่วยนอก (OP)? ก. 2 บาท/เม็ด ข. 3 บาท/เม็ด ค. 4 บาท/เม็ด ง. 5 บาท/เม็ด\"\n",
    "    \n",
    "    \"\"\"Main function with enhanced document loading options\"\"\"\n",
    "    qa_system = ThaiQASystem()\n",
    "    \n",
    "    try:\n",
    "        # Load model\n",
    "        qa_system.load_model()\n",
    "        qa_system.load_multiple_documents([\n",
    "            \"./data/doc_csv.csv\", \n",
    "            \"./data/labeled_data.csv\",\n",
    "        ])\n",
    "        # ans = qa_system.get_answer_only(question)\n",
    "        # print(ans)\n",
    "        qa_system.generate_submission_file(\"./data/test.csv\", \"./data/submission.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ff4f1f2-c199-4038-8983-a9f636c40bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54475224976548448c27c09a684ecd41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully!\n",
      "📂 Found 2 document files to load\n",
      "📖 Loading: ./data/doc_csv.csv\n",
      "  📋 Detected MD format in doc_csv.csv\n",
      "  ✅ Loaded 376 entries from doc_csv.csv\n",
      "📖 Loading: ./data/labeled_data.csv\n",
      "  📋 Detected Cheatsheet format in labeled_data.csv\n",
      "  ✅ Loaded 432 entries from labeled_data.csv\n",
      "📚 Total documents loaded: 808\n",
      "🔄 Creating new embeddings...\n",
      "🔄 Loading embedder...\n",
      "✅ Embedder loaded successfully!\n",
      "🔄 Creating embeddings for 808 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings:   0%|                                                                                                                                    | 0/26 [00:00<?, ?it/s]/home/siamai/tatar/cmkl-med-hackathon/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Creating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 46.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Combined documents saved to combined_docs_with_embeddings.json\n",
      "✅ Embeddings created for 808 documents\n",
      "📖 Reading test file: ./data/test.csv\n",
      "📊 Processing 500 questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating answers:   0%|                                                                                                                                 | 0/500 [00:00<?, ?it/s]/home/siamai/tatar/cmkl-med-hackathon/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "🧠 Generating answers:   0%|▏                                                                                                                      | 1/500 [00:21<3:00:40, 21.72s/it]/home/siamai/tatar/cmkl-med-hackathon/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "🧠 Generating answers:   0%|▍                                                                                                                      | 2/500 [00:42<2:55:48, 21.18s/it]/home/siamai/tatar/cmkl-med-hackathon/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "🧠 Generating answers:   2%|██▋                                                                                                                     | 11/500 [01:04<55:52,  6.86s/it]/home/siamai/tatar/cmkl-med-hackathon/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "🧠 Generating answers:   3%|███▊                                                                                                                  | 16/500 [01:25<1:00:23,  7.49s/it]/home/siamai/tatar/cmkl-med-hackathon/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "🧠 Generating answers:   4%|████▍                                                                                                                 | 19/500 [01:46<1:09:52,  8.72s/it]/home/siamai/tatar/cmkl-med-hackathon/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "🧠 Generating answers:  10%|████████████▏                                                                                                           | 51/500 [02:11<48:10,  6.44s/it]/home/siamai/tatar/cmkl-med-hackathon/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "🧠 Generating answers:  16%|██████████████████▋                                                                                                     | 78/500 [02:36<45:10,  6.42s/it]/home/siamai/tatar/cmkl-med-hackathon/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "🧠 Generating answers:  33%|██████████████████████████████████████▊                                                                                | 163/500 [03:10<36:28,  6.49s/it]/home/siamai/tatar/cmkl-med-hackathon/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "🧠 Generating answers: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [03:59<00:00,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Submission file created: ./data/submission.csv\n",
      "\n",
      "📋 Sample results:\n",
      "   id answer\n",
      "0   1    \"ค\"\n",
      "1   2    \"ข\"\n",
      "2   3    \"ก\"\n",
      "3   4    \"ง\"\n",
      "4   5    \"ค\"\n",
      "5   6    \"ง\"\n",
      "6   7    \"ง\"\n",
      "7   8    \"ข\"\n",
      "8   9    \"ก\"\n",
      "9  10    \"ข\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "788776cd-a2bc-404d-b877-673d7a03b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/submission_1.csv')\n",
    "\n",
    "# ล้าง triple quotes ถ้ามี\n",
    "df['answer'] = df['answer'].astype(str).str.strip('\"')\n",
    "\n",
    "# ใส่ double quotes รอบ answer เอง\n",
    "df['answer'] = '\"' + df['answer'] + '\"'\n",
    "\n",
    "# เขียนไฟล์ใหม่โดยไม่ใช้ quoting ของ pandas\n",
    "df.to_csv('./data/submission.csv', index=False, quoting=3)  # 3 คือ csv.QUOTE_NONE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89b3713f-ff84-4ceb-b97f-0f8273fc8241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: 229/500 (45.80%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(45.800000000000004)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sim(csv1, csv2):\n",
    "    # โหลดไฟล์ CSV\n",
    "    df1 = pd.read_csv(csv1)\n",
    "    df2 = pd.read_csv(csv2)\n",
    "\n",
    "    # รวมข้อมูลโดยอิงตาม id ที่ตรงกันเท่านั้น\n",
    "    merged = pd.merge(df1, df2, on='id', suffixes=('_1', '_2'))\n",
    "\n",
    "    # ลบ whitespace และ quote หากยังมี\n",
    "    merged['answer_1'] = merged['answer_1'].astype(str).str.strip().str.strip('\"')\n",
    "    merged['answer_2'] = merged['answer_2'].astype(str).str.strip().str.strip('\"')\n",
    "\n",
    "    # เปรียบเทียบคำตอบ\n",
    "    total = len(merged)\n",
    "    correct = (merged['answer_1'] == merged['answer_2']).sum()\n",
    "\n",
    "    # คำนวณเปอร์เซ็นต์ความเหมือน\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "\n",
    "    print(f\"Matched: {correct}/{total} ({accuracy:.2f}%)\")\n",
    "    return accuracy\n",
    "\n",
    "sim('./data/s1.csv', './data/s2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b17ec15-34fe-4f66-8810-51748a954d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
