{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19f46fc3-1a83-474a-a8da-6033ea5effed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from typing import Optional, List, Dict, Any, Union\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609b3b3-cdcf-4b23-b353-a03f18ce2348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThaiQASystem:\n",
    "    def __init__(self, model_id: str = \"scb10x/typhoon2.1-gemma3-4b\"):\n",
    "        \"\"\"Initialize the Thai Q&A System\"\"\"\n",
    "        self.model_id = model_id\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.embedder = None\n",
    "        self.doc_embeddings = None\n",
    "        self.doc_df = None\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the language model\"\"\"\n",
    "        print(\"üîÑ Loading model...\")\n",
    "        try:\n",
    "            torch._dynamo.config.cache_size_limit = 1024\n",
    "            torch.set_float32_matmul_precision('high')\n",
    "            \n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_id,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "            print(\"‚úÖ Model loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_embedder(self, embedder_model: str = \"BAAI/bge-m3\"):\n",
    "        \"\"\"Load sentence transformer for RAG\"\"\"\n",
    "        print(\"üîÑ Loading embedder...\")\n",
    "        try:\n",
    "            self.embedder = SentenceTransformer(embedder_model)\n",
    "            print(\"‚úÖ Embedder loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading embedder: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_multiple_documents(self, \n",
    "                              doc_paths: Union[str, List[str]], \n",
    "                              embedding_path: Optional[str] = None,\n",
    "                              force_recreate_embeddings: bool = False):\n",
    "        \"\"\"\n",
    "        Load multiple documents from various sources\n",
    "        \n",
    "        Args:\n",
    "            doc_paths: Single path, list of paths, or directory path with wildcards\n",
    "            embedding_path: Path to save/load embeddings\n",
    "            force_recreate_embeddings: Force recreation of embeddings even if they exist\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Handle different input types\n",
    "            if isinstance(doc_paths, str):\n",
    "                if '*' in doc_paths or '?' in doc_paths:\n",
    "                    # Handle wildcards\n",
    "                    doc_files = glob.glob(doc_paths)\n",
    "                elif os.path.isdir(doc_paths):\n",
    "                    # Handle directory - get all CSV files\n",
    "                    doc_files = glob.glob(os.path.join(doc_paths, \"*.csv\"))\n",
    "                else:\n",
    "                    # Single file\n",
    "                    doc_files = [doc_paths]\n",
    "            else:\n",
    "                # List of files\n",
    "                doc_files = doc_paths\n",
    "            \n",
    "            print(f\"üìÇ Found {len(doc_files)} document files to load\")\n",
    "            \n",
    "            # Load and combine all documents\n",
    "            all_docs = []\n",
    "            for doc_path in doc_files:\n",
    "                print(f\"üìñ Loading: {doc_path}\")\n",
    "                \n",
    "                if doc_path.endswith('.json'):\n",
    "                    df = pd.read_json(doc_path, lines=True)\n",
    "                else:\n",
    "                    df = pd.read_csv(doc_path, encoding='utf-8')\n",
    "                \n",
    "                # Process based on file type/structure\n",
    "                processed_df = self._process_document_structure(df, doc_path)\n",
    "                all_docs.append(processed_df)\n",
    "                print(f\"  ‚úÖ Loaded {len(processed_df)} entries from {os.path.basename(doc_path)}\")\n",
    "            \n",
    "            # Combine all documents\n",
    "            if all_docs:\n",
    "                self.doc_df = pd.concat(all_docs, ignore_index=True)\n",
    "                print(f\"üìö Total documents loaded: {len(self.doc_df)}\")\n",
    "                \n",
    "                # Load or create embeddings\n",
    "                if embedding_path and os.path.exists(embedding_path) and not force_recreate_embeddings:\n",
    "                    print(\"üì• Loading existing embeddings...\")\n",
    "                    self.doc_embeddings = np.load(embedding_path)\n",
    "                    if len(self.doc_embeddings) != len(self.doc_df):\n",
    "                        print(\"‚ö†Ô∏è Embedding count mismatch, recreating...\")\n",
    "                        self.create_embeddings(save_path=embedding_path)\n",
    "                    else:\n",
    "                        print(\"‚úÖ Embeddings loaded successfully!\")\n",
    "                else:\n",
    "                    print(\"üîÑ Creating new embeddings...\")\n",
    "                    self.create_embeddings(save_path=embedding_path)\n",
    "            else:\n",
    "                print(\"‚ùå No documents loaded!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading documents: {e}\")\n",
    "            self.doc_df = None\n",
    "            self.doc_embeddings = None\n",
    "    \n",
    "    def _process_document_structure(self, df: pd.DataFrame, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process different document structures and normalize them\n",
    "        \"\"\"\n",
    "        file_name = os.path.basename(file_path).lower()\n",
    "        \n",
    "        # Check for md_csv format: [source_document, page_number, content_type, content]\n",
    "        if all(col in df.columns for col in ['source_document', 'page_number', 'content_type', 'content']):\n",
    "            print(f\"  üìã Detected MD format in {file_name}\")\n",
    "            processed_df = pd.DataFrame({\n",
    "                'content': df['content'].astype(str),\n",
    "                'source': df['source_document'].astype(str),\n",
    "                'page': df['page_number'].astype(str),\n",
    "                'type': df['content_type'].astype(str),\n",
    "                'file_origin': file_name\n",
    "            })\n",
    "            \n",
    "        # Check for cheatsheet_csv format: [Question No, Question, Answer, Explanation]\n",
    "        elif all(col in df.columns for col in ['Question No', 'Question', 'Answer', 'Explanation']):\n",
    "            print(f\"  üìã Detected Cheatsheet format in {file_name}\")\n",
    "            # Combine question, answer, and explanation into content\n",
    "            df['combined_content'] = (\n",
    "                \"‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: \" + df['Question'].astype(str) + \n",
    "                \" ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: \" + df['Answer'].astype(str) + \n",
    "                \" ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: \" + df['Explanation'].astype(str)\n",
    "            )\n",
    "            processed_df = pd.DataFrame({\n",
    "                'content': df['combined_content'],\n",
    "                'source': 'cheatsheet',\n",
    "                'page': df['Question No'].astype(str),\n",
    "                'type': 'qa_pair',\n",
    "                'file_origin': file_name\n",
    "            })\n",
    "            \n",
    "        # Check for simple content format\n",
    "        elif 'content' in df.columns:\n",
    "            print(f\"  üìã Detected simple content format in {file_name}\")\n",
    "            processed_df = pd.DataFrame({\n",
    "                'content': df['content'].astype(str),\n",
    "                'source': df.get('source', 'unknown').astype(str) if 'source' in df.columns else 'unknown',\n",
    "                'page': df.get('page', '1').astype(str) if 'page' in df.columns else '1',\n",
    "                'type': df.get('type', 'document').astype(str) if 'type' in df.columns else 'document',\n",
    "                'file_origin': file_name\n",
    "            })\n",
    "            \n",
    "        # Auto-detect based on column names\n",
    "        else:\n",
    "            print(f\"  üìã Auto-detecting format in {file_name}\")\n",
    "            # Use first text column as content\n",
    "            text_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "            if text_columns:\n",
    "                content_col = text_columns[0]\n",
    "                processed_df = pd.DataFrame({\n",
    "                    'content': df[content_col].astype(str),\n",
    "                    'source': 'auto_detected',\n",
    "                    'page': '1',\n",
    "                    'type': 'document',\n",
    "                    'file_origin': file_name\n",
    "                })\n",
    "            else:\n",
    "                raise ValueError(f\"No suitable text columns found in {file_name}\")\n",
    "        \n",
    "        # Clean and filter content\n",
    "        processed_df = processed_df[processed_df['content'].str.strip() != '']\n",
    "        processed_df = processed_df[processed_df['content'] != 'nan']\n",
    "        processed_df['content'] = processed_df['content'].str.strip()\n",
    "        \n",
    "        return processed_df\n",
    "    \n",
    "    def create_embeddings(self, save_path: str = \"embeddings_multi_docs.npy\"):\n",
    "        \"\"\"Create embeddings for documents\"\"\"\n",
    "        if self.embedder is None:\n",
    "            self.load_embedder()\n",
    "        \n",
    "        if self.doc_df is None or len(self.doc_df) == 0:\n",
    "            print(\"‚ùå No documents loaded!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"üîÑ Creating embeddings for {len(self.doc_df)} documents...\")\n",
    "        texts = [\"passage: \" + str(x) for x in self.doc_df[\"content\"]]\n",
    "        \n",
    "        # Create embeddings in batches to handle large datasets\n",
    "        batch_size = 32\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Creating embeddings\"):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.embedder.encode(batch_texts, batch_size=len(batch_texts))\n",
    "            all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        self.doc_embeddings = np.vstack(all_embeddings)\n",
    "        \n",
    "        # Save embeddings\n",
    "        if save_path:\n",
    "            np.save(save_path, self.doc_embeddings)\n",
    "            print(f\"üíæ Embeddings saved to {save_path}\")\n",
    "        \n",
    "        # Save dataframe with embeddings\n",
    "        self.doc_df[\"embedding\"] = self.doc_embeddings.tolist()\n",
    "        combined_df_path = \"combined_docs_with_embeddings.json\"\n",
    "        self.doc_df.to_json(combined_df_path, orient=\"records\", lines=True)\n",
    "        print(f\"üíæ Combined documents saved to {combined_df_path}\")\n",
    "        \n",
    "        print(f\"‚úÖ Embeddings created for {len(self.doc_embeddings)} documents\")\n",
    "    \n",
    "    def retrieve_relevant_docs(self, question: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve relevant documents with metadata\"\"\"\n",
    "        if self.embedder is None or self.doc_embeddings is None:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Encode question\n",
    "            question_embedding = self.embedder.encode([\"query: \" + question])\n",
    "            \n",
    "            # Calculate similarities\n",
    "            similarities = np.dot(self.doc_embeddings, question_embedding.T).flatten()\n",
    "            top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "            \n",
    "            # Get relevant documents with metadata\n",
    "            relevant_docs = []\n",
    "            for idx in top_indices:\n",
    "                if idx < len(self.doc_df):\n",
    "                    doc_info = {\n",
    "                        'content': str(self.doc_df.iloc[idx][\"content\"]),\n",
    "                        'source': str(self.doc_df.iloc[idx][\"source\"]),\n",
    "                        'page': str(self.doc_df.iloc[idx][\"page\"]),\n",
    "                        'type': str(self.doc_df.iloc[idx][\"type\"]),\n",
    "                        'file_origin': str(self.doc_df.iloc[idx][\"file_origin\"]),\n",
    "                        'similarity': float(similarities[idx])\n",
    "                    }\n",
    "                    relevant_docs.append(doc_info)\n",
    "            \n",
    "            return relevant_docs\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error in document retrieval: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_answer_only(self, question: str, use_rag: bool = True, top_k: int = 3) -> str:\n",
    "        \"\"\"Get answer from model with optional RAG\"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            raise Exception(\"Model not loaded! Call load_model() first.\")\n",
    "        \n",
    "        # Prepare context if RAG is enabled\n",
    "        context = \"\"\n",
    "        if use_rag and self.doc_embeddings is not None:\n",
    "            relevant_docs = self.retrieve_relevant_docs(question, top_k=top_k)\n",
    "            if relevant_docs:\n",
    "                context_parts = []\n",
    "                for i, doc in enumerate(relevant_docs[:top_k]):\n",
    "                    source_info = f\"[{doc['file_origin']}:{doc['source']}:{doc['page']}]\"\n",
    "                    context_parts.append(f\"{source_info} {doc['content']}\")\n",
    "                \n",
    "                context = \"\\n\\n‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:\\n\" + \"\\n\".join(context_parts)\n",
    "        \n",
    "        # Prepare messages\n",
    "        system_prompt = (\n",
    "            \"You are an AI that answers multiple choice questions in Thai. \"\n",
    "            \"Reply only with a valid JSON object in this exact format: \"\n",
    "            '{\"answer\": \"‡∏Å\"}. Do not include any explanation. '\n",
    "            'Choices must be enclosed in double quotes. Do not add anything else outside the JSON.'\n",
    "        )\n",
    "        \n",
    "        if context:\n",
    "            system_prompt += \"\\n\\nUse the provided context to help answer the question.\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question.strip() + context}\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            input_ids = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.model.device)\n",
    "\n",
    "            outputs = self.model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=128,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.95,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            response = outputs[0][input_ids.shape[-1]:]\n",
    "            decoded = self.tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "            # Extract JSON\n",
    "            try:\n",
    "                # Find JSON in response\n",
    "                start_idx = decoded.find(\"{\")\n",
    "                end_idx = decoded.rfind(\"}\") + 1\n",
    "                \n",
    "                if start_idx != -1 and end_idx > start_idx:\n",
    "                    json_str = decoded[start_idx:end_idx]\n",
    "                    result = json.loads(json_str)\n",
    "                    answer = result.get(\"answer\", \"\").strip()\n",
    "                    # Remove quotes if present\n",
    "                    return answer.strip('\"')\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è No JSON found in: {decoded}\")\n",
    "                    return \"\"\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"‚ö†Ô∏è JSON decode error: {e}\")\n",
    "                print(f\"Raw output: {decoded}\")\n",
    "                return \"\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Generation error: {e}\")\n",
    "            return \"\"\n",
    "    def get_answer_and_reason(self, question: str):\n",
    "        # Stub: Replace with actual retrieval + reasoning logic\n",
    "        if \"Clopidogrel\" in question:\n",
    "            return \"‡∏Ç\", \"‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô PDF ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤ Clopidogrel mg tablet OP: ‡πÄ‡∏ö‡∏¥‡∏Å‡πÑ‡∏î‡πâ 3 ‡∏ö‡∏≤‡∏ó/‡πÄ‡∏°‡πá‡∏î\"\n",
    "        else:\n",
    "            return \"‡∏á\", \"‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡πÉ‡∏ô PDF ‡∏à‡∏∂‡∏á‡∏ï‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠ ‡∏á ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏õ‡∏£‡∏¥‡∏¢‡∏≤‡∏¢\"\n",
    "\n",
    "    def test_single_question(self, question: str, use_rag: bool = True, top_k: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"Test a single question with detailed info\"\"\"\n",
    "        print(f\"\\nü§î ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}\")\n",
    "        \n",
    "        relevant_docs = []\n",
    "        if use_rag and self.doc_embeddings is not None:\n",
    "            relevant_docs = self.retrieve_relevant_docs(question, top_k=top_k)\n",
    "            print(f\"üìö ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: {len(relevant_docs)} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£\")\n",
    "            \n",
    "            # Show top relevant documents\n",
    "            for i, doc in enumerate(relevant_docs[:3]):\n",
    "                print(f\"  {i+1}. [{doc['file_origin']}] similarity: {doc['similarity']:.3f}\")\n",
    "                print(f\"     {doc['content'][:100]}...\")\n",
    "        \n",
    "        answer = self.get_answer_only(question, use_rag=use_rag, top_k=top_k)\n",
    "        print(f\"üí° ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {answer}\")\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"used_rag\": use_rag,\n",
    "            \"relevant_docs\": relevant_docs\n",
    "        }\n",
    "\n",
    "    def show_document_stats(self):\n",
    "        \"\"\"Show statistics about loaded documents\"\"\"\n",
    "        if self.doc_df is None:\n",
    "            print(\"‚ùå No documents loaded!\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nüìä Document Statistics:\")\n",
    "        print(f\"Total documents: {len(self.doc_df)}\")\n",
    "        print(f\"Document types: {self.doc_df['type'].value_counts().to_dict()}\")\n",
    "        print(f\"Source files: {self.doc_df['file_origin'].value_counts().to_dict()}\")\n",
    "        print(f\"Sources: {self.doc_df['source'].nunique()} unique sources\")\n",
    "        \n",
    "        # Show sample content\n",
    "        print(\"\\nüìñ Sample content:\")\n",
    "        for i in range(min(3, len(self.doc_df))):\n",
    "            doc = self.doc_df.iloc[i]\n",
    "            print(f\"{i+1}. [{doc['file_origin']}:{doc['type']}] {doc['content'][:100]}...\")\n",
    "\n",
    "    def generate_submission_file(self, test_file_path: str, submission_file_path: str, \n",
    "                               use_rag: bool = True, top_k: int = 3):\n",
    "        \"\"\"Generate submission file from test CSV\"\"\"\n",
    "        try:\n",
    "            print(f\"üìñ Reading test file: {test_file_path}\")\n",
    "            test_df = pd.read_csv(test_file_path, encoding='utf-8')\n",
    "            \n",
    "            if 'id' not in test_df.columns or 'question' not in test_df.columns:\n",
    "                raise ValueError(\"‚ùå Test file must have 'id' and 'question' columns\")\n",
    "            \n",
    "            print(f\"üìä Processing {len(test_df)} questions...\")\n",
    "            \n",
    "            submission_rows = []\n",
    "            for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"üß† Generating answers\"):\n",
    "                answer = self.get_answer_only(row['question'], use_rag=use_rag, top_k=top_k)\n",
    "                submission_rows.append({\n",
    "                    \"id\": row['id'],\n",
    "                    \"answer\": f'\"{answer}\"' if answer and not answer.startswith('\"') else answer\n",
    "                })\n",
    "\n",
    "            submission_df = pd.DataFrame(submission_rows)\n",
    "            submission_df.to_csv(submission_file_path, index=False, encoding='utf-8')\n",
    "            print(f\"‚úÖ Submission file created: {submission_file_path}\")\n",
    "            \n",
    "            # Show sample results\n",
    "            print(\"\\nüìã Sample results:\")\n",
    "            print(submission_df.head(10))\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ùå Test file not found: {test_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generating submission: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c7e73b-f4f7-4ede-a1bc-00133cf81d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    question = \"‡∏¢‡∏≤ Clopidogrel mg tablet ‡πÉ‡∏ô‡∏õ‡∏µ 2567 ‡∏à‡πà‡∏≤‡∏¢‡πÉ‡∏ô‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡πÉ‡∏î‡∏ï‡πà‡∏≠‡πÄ‡∏°‡πá‡∏î‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏ô‡∏≠‡∏Å (OP)? ‡∏Å. 2 ‡∏ö‡∏≤‡∏ó/‡πÄ‡∏°‡πá‡∏î ‡∏Ç. 3 ‡∏ö‡∏≤‡∏ó/‡πÄ‡∏°‡πá‡∏î ‡∏Ñ. 4 ‡∏ö‡∏≤‡∏ó/‡πÄ‡∏°‡πá‡∏î ‡∏á. 5 ‡∏ö‡∏≤‡∏ó/‡πÄ‡∏°‡πá‡∏î\"\n",
    "    \n",
    "    \"\"\"Main function with enhanced document loading options\"\"\"\n",
    "    qa_system = ThaiQASystem()\n",
    "    \n",
    "    try:\n",
    "        # Load model\n",
    "        qa_system.load_model()\n",
    "        qa_system.load_multiple_documents([\n",
    "            \"./data/doc_csv.csv\", \n",
    "            \"./data/labeled_data.csv\",\n",
    "        ])\n",
    "        # ans = qa_system.get_answer_only(question)\n",
    "        # print(ans)\n",
    "        qa_system.generate_submission_file(\"./data/test.csv\", \"./data/submission.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ff4f1f2-c199-4038-8983-a9f636c40bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54475224976548448c27c09a684ecd41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "üìÇ Found 2 document files to load\n",
      "üìñ Loading: ./data/doc_csv.csv\n",
      "  üìã Detected MD format in doc_csv.csv\n",
      "  ‚úÖ Loaded 376 entries from doc_csv.csv\n",
      "üìñ Loading: ./data/labeled_data.csv\n",
      "  üìã Detected Cheatsheet format in labeled_data.csv\n",
      "  ‚úÖ Loaded 432 entries from labeled_data.csv\n",
      "üìö Total documents loaded: 808\n",
      "üîÑ Creating new embeddings...\n",
      "üîÑ Loading embedder...\n",
      "‚úÖ Embedder loaded successfully!\n",
      "üîÑ Creating embeddings for 808 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings:   0%|                                                                                                                                    | 0/26 [00:00<?, ?it/s]/home/siamai/tatar/cmkl-med-hackathon/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Creating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:00<00:00, 46.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Combined documents saved to combined_docs_with_embeddings.json\n",
      "‚úÖ Embeddings created for 808 documents\n",
      "üìñ Reading test file: ./data/test.csv\n",
      "üìä Processing 500 questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating answers:   0%|                                                                                                                                 | 0/500 [00:00<?, ?it/s]/home/siamai/tatar/cmkl-med-hackathon/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "üß† Generating answers:   0%|‚ñè                                                                                                                      | 1/500 [00:21<3:00:40, 21.72s/it]/home/siamai/tatar/cmkl-med-hackathon/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "üß† Generating answers:   0%|‚ñç                                                                                                                      | 2/500 [00:42<2:55:48, 21.18s/it]/home/siamai/tatar/cmkl-med-hackathon/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "üß† Generating answers:   2%|‚ñà‚ñà‚ñã                                                                                                                     | 11/500 [01:04<55:52,  6.86s/it]/home/siamai/tatar/cmkl-med-hackathon/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "üß† Generating answers:   3%|‚ñà‚ñà‚ñà‚ñä                                                                                                                  | 16/500 [01:25<1:00:23,  7.49s/it]/home/siamai/tatar/cmkl-med-hackathon/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "üß† Generating answers:   4%|‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                                                 | 19/500 [01:46<1:09:52,  8.72s/it]/home/siamai/tatar/cmkl-med-hackathon/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "üß† Generating answers:  10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                                           | 51/500 [02:11<48:10,  6.44s/it]/home/siamai/tatar/cmkl-med-hackathon/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "üß† Generating answers:  16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                                                     | 78/500 [02:36<45:10,  6.42s/it]/home/siamai/tatar/cmkl-med-hackathon/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "üß† Generating answers:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                | 163/500 [03:10<36:28,  6.49s/it]/home/siamai/tatar/cmkl-med-hackathon/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "üß† Generating answers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [03:59<00:00,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Submission file created: ./data/submission.csv\n",
      "\n",
      "üìã Sample results:\n",
      "   id answer\n",
      "0   1    \"‡∏Ñ\"\n",
      "1   2    \"‡∏Ç\"\n",
      "2   3    \"‡∏Å\"\n",
      "3   4    \"‡∏á\"\n",
      "4   5    \"‡∏Ñ\"\n",
      "5   6    \"‡∏á\"\n",
      "6   7    \"‡∏á\"\n",
      "7   8    \"‡∏Ç\"\n",
      "8   9    \"‡∏Å\"\n",
      "9  10    \"‡∏Ç\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "788776cd-a2bc-404d-b877-673d7a03b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/submission_1.csv')\n",
    "\n",
    "# ‡∏•‡πâ‡∏≤‡∏á triple quotes ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ\n",
    "df['answer'] = df['answer'].astype(str).str.strip('\"')\n",
    "\n",
    "# ‡πÉ‡∏™‡πà double quotes ‡∏£‡∏≠‡∏ö answer ‡πÄ‡∏≠‡∏á\n",
    "df['answer'] = '\"' + df['answer'] + '\"'\n",
    "\n",
    "# ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ quoting ‡∏Ç‡∏≠‡∏á pandas\n",
    "df.to_csv('./data/submission.csv', index=False, quoting=3)  # 3 ‡∏Ñ‡∏∑‡∏≠ csv.QUOTE_NONE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89b3713f-ff84-4ceb-b97f-0f8273fc8241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: 229/500 (45.80%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(45.800000000000004)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sim(csv1, csv2):\n",
    "    # ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå CSV\n",
    "    df1 = pd.read_csv(csv1)\n",
    "    df2 = pd.read_csv(csv2)\n",
    "\n",
    "    # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏ï‡∏≤‡∏° id ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n",
    "    merged = pd.merge(df1, df2, on='id', suffixes=('_1', '_2'))\n",
    "\n",
    "    # ‡∏•‡∏ö whitespace ‡πÅ‡∏•‡∏∞ quote ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡∏°‡∏µ\n",
    "    merged['answer_1'] = merged['answer_1'].astype(str).str.strip().str.strip('\"')\n",
    "    merged['answer_2'] = merged['answer_2'].astype(str).str.strip().str.strip('\"')\n",
    "\n",
    "    # ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö\n",
    "    total = len(merged)\n",
    "    correct = (merged['answer_1'] == merged['answer_2']).sum()\n",
    "\n",
    "    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "\n",
    "    print(f\"Matched: {correct}/{total} ({accuracy:.2f}%)\")\n",
    "    return accuracy\n",
    "\n",
    "sim('./data/s1.csv', './data/s2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b17ec15-34fe-4f66-8810-51748a954d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
